{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIjoPwkoLfxf"
      },
      "source": [
        "# ENV Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2KKtT8WLjT_",
        "outputId": "252a6bea-938a-4f62-8ba7-6844755d2086"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.454s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found 2 \u001b[93mmoderate\u001b[0m severity vulnerabilities\n",
            "  run `npm audit fix` to fix them, or `npm audit` for details\n",
            "\u001b[K\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# TGI ependencies\n",
        "!pip install -q text-generation \\\n",
        "                langchain \\\n",
        "                transformers\n",
        "# Chatbot dependencies\n",
        "!pip install -q streamlit \\\n",
        "                streamlit-chat \\\n",
        "                langchain \\\n",
        "                langchainhub \\\n",
        "                huggingface_hub \\\n",
        "                transformers \\\n",
        "                pypdf \\\n",
        "                sentence_transformers \\\n",
        "                chromadb \\\n",
        "                tiktoken\n",
        "# Streamlit UI Dependencies\n",
        "!npm install -q localtunnel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RXbSx2F-_1m"
      },
      "source": [
        "# Ingest.py File\n",
        "Script for ingesting PDF and converting it into vector database"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXFS3kANAowT",
        "outputId": "bcf51e8f-f24d-4d7e-fea4-5209d51f9b21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting ingest.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile ingest.py\n",
        "\n",
        "# Ingest packages\n",
        "import os\n",
        "import torch\n",
        "from dotenv import load_dotenv\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain import HuggingFaceHub\n",
        "\n",
        "\n",
        "# Tokenizer\n",
        "embedd_model = 'BAAI/bge-reranker-large'\n",
        "model_kwargs = {\"device\": 'cuda'}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=embedd_model, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "def ingest_doc(doc_path, file_name):\n",
        "\n",
        "    # Checking if vector database exists, creating it if not\n",
        "    outdir = \"./backend/vector_databases/\"\n",
        "    if not os.path.exists(outdir):\n",
        "        os.makedirs(outdir)\n",
        "\n",
        "    # Creating database path\n",
        "    db_path = os.path.join(outdir, file_name)\n",
        "    print('Db Path: ', db_path)\n",
        "\n",
        "    # Checking if the database already exists, and creating it if it doesn't\n",
        "    if not os.path.exists(db_path):\n",
        "        # Loading doc\n",
        "        loader = PyPDFLoader(doc_path)\n",
        "        raw_doc = loader.load()\n",
        "\n",
        "        # Split and store vectors\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
        "                                                    chunk_overlap=0,\n",
        "                                                    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"])\n",
        "        all_splits = text_splitter.split_documents(raw_doc)\n",
        "\n",
        "\n",
        "        # Creating vector store\n",
        "        vectorstore = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=db_path)\n",
        "    else:\n",
        "        vectorstore = Chroma(persist_directory=db_path, embedding_function=embeddings)\n",
        "\n",
        "    return vectorstore\n",
        "\n",
        "def create_doc_obj(doc_path, file_name):\n",
        "\n",
        "    # Checking if vector database exists, creating it if not\n",
        "    outdir = \"./backend/vector_databases/\"\n",
        "    if not os.path.exists(outdir):\n",
        "        os.makedirs(outdir)\n",
        "\n",
        "    # Creating database path\n",
        "    db_path = os.path.join(outdir, file_name)\n",
        "    print('Db Path: ', db_path)\n",
        "\n",
        "    # Creating document object\n",
        "    loader = PyPDFLoader(doc_path)\n",
        "    raw_doc = loader.load()\n",
        "\n",
        "    return raw_doc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HdnBeHrTx1K3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCxfh54zIsiC"
      },
      "source": [
        "# Core.py File\n",
        "Script for querrying the LLM and returning response/source documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOQ4VLnkIrm_",
        "outputId": "5614cf88-bd52-4bb8-9529-f7e27c3e3f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting core.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile core.py\n",
        "\n",
        "\n",
        "from typing import Any, List, Dict\n",
        "\n",
        "#Summary and checklist packages\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "# Chat packages\n",
        "import torch\n",
        "import os\n",
        "from langchain.llms import HuggingFaceTextGenInference\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "from langchain.callbacks import streaming_stdout\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import hub\n",
        "\n",
        "\n",
        "# Global variables\n",
        "huggingfacehub_api_token = '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'\n",
        "zephyr_repo = 'HuggingFaceH4/zephyr-7b-beta'\n",
        "rag_prompt = hub.pull(\"rlm/rag-prompt-mistral\")\n",
        "\n",
        "\n",
        "# TGI URL\n",
        "inference_url = \"https://wp021uax7a.execute-api.us-west-2.amazonaws.com/default/mistralrequest\"\n",
        "\n",
        "# Tokenizer\n",
        "embedd_model = 'BAAI/bge-reranker-large'\n",
        "model_kwargs = {\"device\": 'cuda'}\n",
        "encode_kwargs = {\"normalize_embeddings\": True}\n",
        "embeddings = HuggingFaceBgeEmbeddings(\n",
        "    model_name=embedd_model, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs\n",
        ")\n",
        "\n",
        "\n",
        "# Chat LLM\n",
        "chat_llm = HuggingFaceTextGenInference(\n",
        "    inference_server_url=inference_url,\n",
        "    max_new_tokens=512,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "\n",
        "\n",
        "# Summarization and checklist LLM\n",
        "sum_check_llm = HuggingFaceTextGenInference(\n",
        "    inference_server_url=inference_url,\n",
        "    max_new_tokens=1000,\n",
        "    top_k=10,\n",
        "    top_p=0.95,\n",
        "    typical_p=0.95,\n",
        "    temperature=0.01,\n",
        "    repetition_penalty=1.03,\n",
        ")\n",
        "\n",
        "\n",
        "# Call LLM for summary\n",
        "def run_llm_summarize(document_object: Any):\n",
        "\n",
        "    docs = document_object\n",
        "\n",
        "    # Map\n",
        "    map_template = \"\"\"<s> [INST] The following is a collection of excerpts from a compliance document:[/INST] </s>\n",
        "    {docs}\n",
        "    [INST] Based on the provided excerpts, summarize the main theme.\n",
        "    Helpful Answer:[/INST]\"\"\"\n",
        "    map_prompt = PromptTemplate.from_template(map_template)\n",
        "    map_chain = LLMChain(llm=sum_check_llm, prompt=map_prompt)\n",
        "\n",
        "    # Reduce\n",
        "    reduce_template = \"\"\"<s> [INST] The following is set of summaries:[/INST] </s>\n",
        "    {doc_summaries}\n",
        "    [INST] Take these and distill it into a final, consolidated summary. Ensure the final output is concise and easy to read.\n",
        "    Helpful Answer:[/INST]\"\"\"\n",
        "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "    reduce_chain = LLMChain(llm=sum_check_llm, prompt=reduce_prompt)\n",
        "\n",
        "    # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
        "    combine_documents_chain = StuffDocumentsChain(\n",
        "        llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
        "    )\n",
        "\n",
        "    # Combines and iteravely reduces the mapped documents\n",
        "    reduce_documents_chain = ReduceDocumentsChain(\n",
        "        # This is final chain that is called.\n",
        "        combine_documents_chain=combine_documents_chain,\n",
        "        # If documents exceed context for `StuffDocumentsChain`\n",
        "        collapse_documents_chain=combine_documents_chain,\n",
        "        # The maximum number of tokens to group documents into.\n",
        "        token_max=4000,\n",
        "    )\n",
        "\n",
        "    # Combining documents by mapping a chain over them, then combining results\n",
        "    map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Map chain\n",
        "    llm_chain=map_chain,\n",
        "    # Reduce chain\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # The variable name in the llm_chain to put the documents in\n",
        "    document_variable_name=\"docs\",\n",
        "    # Return the results of the map steps in the output\n",
        "    return_intermediate_steps=False,\n",
        ")\n",
        "\n",
        "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=1000, chunk_overlap=0)\n",
        "\n",
        "    split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "    summary = map_reduce_chain.run(split_docs)\n",
        "\n",
        "    return summary\n",
        "\n",
        "\n",
        "# Call LLM for checklist\n",
        "def run_llm_checklist(document_object: Any):\n",
        "\n",
        "    docs = document_object\n",
        "\n",
        "    # Map\n",
        "    map_template = \"\"\"<s> [INST] The following is a collection of guidance from a compliance document:[/INST] </s>\n",
        "    {docs}\n",
        "    [INST] Based on the provided guidance, please create a list of suggestions.\n",
        "    Helpful Answer:[/INST]\"\"\"\n",
        "    map_prompt = PromptTemplate.from_template(map_template)\n",
        "    map_chain = LLMChain(llm=sum_check_llm, prompt=map_prompt)\n",
        "\n",
        "    # Reduce\n",
        "    reduce_template = \"\"\"<s> [INST] The following is a colection of suggestions from a compliance document:[/INST] </s>\n",
        "    {doc_summaries}\n",
        "    [INST] Take these and distill them into a final, consolidated list of suggestions to comply with the guidance provided in the document.\n",
        "    Helpful Answer:[/INST]\"\"\"\n",
        "    reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
        "    reduce_chain = LLMChain(llm=sum_check_llm, prompt=reduce_prompt)\n",
        "\n",
        "    # Takes a list of documents, combines them into a single string, and passes this to an LLMChain\n",
        "    combine_documents_chain = StuffDocumentsChain(\n",
        "        llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
        "    )\n",
        "\n",
        "    # Combines and iteravely reduces the mapped documents\n",
        "    reduce_documents_chain = ReduceDocumentsChain(\n",
        "        # This is final chain that is called.\n",
        "        combine_documents_chain=combine_documents_chain,\n",
        "        # If documents exceed context for `StuffDocumentsChain`\n",
        "        collapse_documents_chain=combine_documents_chain,\n",
        "        # The maximum number of tokens to group documents into.\n",
        "        token_max=4000,\n",
        "    )\n",
        "\n",
        "    # Combining documents by mapping a chain over them, then combining results\n",
        "    map_reduce_chain = MapReduceDocumentsChain(\n",
        "    # Map chain\n",
        "    llm_chain=map_chain,\n",
        "    # Reduce chain\n",
        "    reduce_documents_chain=reduce_documents_chain,\n",
        "    # The variable name in the llm_chain to put the documents in\n",
        "    document_variable_name=\"docs\",\n",
        "    # Return the results of the map steps in the output\n",
        "    return_intermediate_steps=False,\n",
        ")\n",
        "\n",
        "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=1000, chunk_overlap=0)\n",
        "\n",
        "    split_docs = text_splitter.split_documents(docs)\n",
        "\n",
        "    suggestion_list = map_reduce_chain.run(split_docs)\n",
        "\n",
        "    return suggestion_list\n",
        "\n",
        "\n",
        "# Call LLM for chat\n",
        "def run_llm_chat(vector_database: Any, question: str):\n",
        "\n",
        "    # Vector DB retriever\n",
        "    retriever = vector_database.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": .8, 'k': 10,})\n",
        "    docs = retriever.get_relevant_documents(question)\n",
        "\n",
        "   # Chain\n",
        "    chain = load_qa_chain(chat_llm, chain_type=\"stuff\", prompt=rag_prompt)\n",
        "    # Run\n",
        "    response = chain({\"input_documents\": docs, \"question\": question}, return_only_outputs=False)\n",
        "\n",
        "    output = response['output_text']\n",
        "\n",
        "    sources = [doc.metadata['page'] for doc in response['input_documents']]\n",
        "    sources.sort()\n",
        "\n",
        "    return output, sources\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "AAJwls19dDls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile utilities.py\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "import streamlit as st\n",
        "from typing import Set\n",
        "from core import run_llm_summarize, run_llm_checklist\n",
        "\n",
        "####################\n",
        "# Utility functions\n",
        "####################\n",
        "\n",
        "# Function to list files in upload directory\n",
        "def list_files():\n",
        "     # Checking if the uploads directory exists, and create it if it doesn't\n",
        "    outdir = \"./backend/uploads/\"\n",
        "    if not os.path.exists(outdir):\n",
        "        os.makedirs(outdir)\n",
        "\n",
        "    return [f for f in os.listdir(outdir) if os.path.isfile(os.path.join(outdir, f))]\n",
        "\n",
        "# Saving a copy of PDF for vectorization\n",
        "def save_upload(file):\n",
        "    file_name = file.name\n",
        "\n",
        "    # Checking if the uploads directory exists, and create it if it doesn't\n",
        "    outdir = \"./backend/uploads/\"\n",
        "    if not os.path.exists(outdir):\n",
        "        os.makedirs(outdir)\n",
        "\n",
        "    # Checking if the file already exists, and saving it if it doesn't\n",
        "    file_path = os.path.join(outdir, file_name)\n",
        "    if not os.path.exists(file_path):\n",
        "        # Saving the file\n",
        "        with open(os.path.join(outdir, file_name), \"wb\") as f:\n",
        "            f.write(file.read())\n",
        "\n",
        "    return file_path, file_name\n",
        "\n",
        "# Return response sources in formatted string\n",
        "def create_sources_string(source_urls: Set[str]) -> str:\n",
        "    if not source_urls:\n",
        "        return \"\"\n",
        "    sources_list = list(source_urls)\n",
        "    sources_list.sort()\n",
        "    sources_string = \"Pulled from pages:\\n\"\n",
        "    for i, source in enumerate(sources_list):\n",
        "        sources_string += f\" {source},\"\n",
        "    return sources_string\n",
        "\n",
        "# Return file name for subheadder\n",
        "@st.cache_resource()\n",
        "def clean_name(doc_name):\n",
        "\n",
        "    cleaned_name = re.sub(r'.pdf', '', doc_name, flags=re.IGNORECASE)\n",
        "    cleaned_name = re.sub(r'\\.', ' ', cleaned_name)\n",
        "    return cleaned_name\n",
        "\n",
        "# Creating or loading summarization\n",
        "@st.cache_data(show_spinner=\"Hey! ðŸ¤–ðŸ‘‹ I'm diving into every page of your document to craft your summary. Depending on how many pages there are, this might take a few minutes. It's the perfect moment to grab yourself a coffee and relax for a bit!\")\n",
        "def create_or_load_summ(_doc_object, doc_name):\n",
        "    # Checking if the uploads directory exists, and create it if it doesn't\n",
        "    outdir = \"./backend/summary/\"\n",
        "    if not os.path.exists(outdir):\n",
        "        os.makedirs(outdir)\n",
        "    file_name = re.sub(r'.pdf', '.txt', doc_name, flags=re.IGNORECASE)\n",
        "    file_path = \"./backend/summary/\"+file_name\n",
        "    # Creating/saving summary if it doesn't exist\n",
        "    if not os.path.exists(file_path):\n",
        "        # Generating summary\n",
        "        summary = run_llm_summarize(document_object=_doc_object)\n",
        "        # Saving to file\n",
        "        with open(file_path, 'w') as file:\n",
        "            file.write(summary)\n",
        "        return summary\n",
        "    # Loading saved summary\n",
        "    else:\n",
        "        with open(file_path, 'r') as file:\n",
        "              summary = file.read()\n",
        "        return summary\n",
        "\n",
        "# Creating or loading checklist\n",
        "@st.cache_data(show_spinner=\"Hi there! ðŸ¤–ðŸ‘‹ I'm currently compiling a list of suggestions based on each page in your document to create your personalized checklist. Depending on the number of pages, this process might take a little while. Feel free to take a break and grab a coffee while I work on this for you!\")\n",
        "def create_or_load_checklist(_doc_object, doc_name):\n",
        "    # Checking if the uploads directory exists, and create it if it doesn't\n",
        "    outdir = \"./backend/checklist/\"\n",
        "    if not os.path.exists(outdir):\n",
        "        os.makedirs(outdir)\n",
        "    file_name = re.sub(r'.pdf', '.txt', doc_name, flags=re.IGNORECASE)\n",
        "    file_path = \"./backend/checklist/\"+file_name\n",
        "    # Creating/saving checklist if it doesn't exist\n",
        "    if not os.path.exists(file_path):\n",
        "        # Generating checklist\n",
        "        checklist = run_llm_checklist(document_object=_doc_object)\n",
        "        # Saving to file\n",
        "        with open(file_path, 'w') as file:\n",
        "            file.write(checklist)\n",
        "        return checklist\n",
        "    # Loading saved checklist\n",
        "    else:\n",
        "        with open(file_path, 'r') as file:\n",
        "              checklist = file.read()\n",
        "        return checklist"
      ],
      "metadata": {
        "id": "TBekR7AKdDFP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ecaad96-69ec-434b-c7e7-e7a6105412ee"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting utilities.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYPMKuO0Ts5Z"
      },
      "source": [
        "# App.py File\n",
        "Script for running the streamlit interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RetfrEPHT0ZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6303e0fb-574f-437a-be65-2a1ebd34a286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import re\n",
        "import time\n",
        "import os\n",
        "import streamlit as st\n",
        "from typing import Set\n",
        "from streamlit_chat import message\n",
        "\n",
        "from ingest import ingest_doc, create_doc_obj\n",
        "from utilities import (list_files,\n",
        "                       save_upload,\n",
        "                       create_sources_string,\n",
        "                       clean_name,\n",
        "                       create_or_load_summ,\n",
        "                       create_or_load_checklist\n",
        "                       )\n",
        "from core import run_llm_summarize, run_llm_checklist, run_llm_chat\n",
        "\n",
        "\n",
        "####################\n",
        "# Utility functions\n",
        "####################\n",
        "\n",
        "# # Function to list files in upload directory\n",
        "# def list_files():\n",
        "#      # Checking if the uploads directory exists, and create it if it doesn't\n",
        "#     outdir = \"./backend/uploads/\"\n",
        "#     if not os.path.exists(outdir):\n",
        "#         os.makedirs(outdir)\n",
        "\n",
        "#     return [f for f in os.listdir(outdir) if os.path.isfile(os.path.join(outdir, f))]\n",
        "\n",
        "# # Saving a copy of PDF for vectorization\n",
        "# def save_upload(file):\n",
        "#     file_name = file.name\n",
        "\n",
        "#     # Checking if the uploads directory exists, and create it if it doesn't\n",
        "#     outdir = \"./backend/uploads/\"\n",
        "#     if not os.path.exists(outdir):\n",
        "#         os.makedirs(outdir)\n",
        "\n",
        "#     # Checking if the file already exists, and saving it if it doesn't\n",
        "#     file_path = os.path.join(outdir, file_name)\n",
        "#     if not os.path.exists(file_path):\n",
        "#         # Saving the file\n",
        "#         with open(os.path.join(outdir, file_name), \"wb\") as f:\n",
        "#             f.write(file.read())\n",
        "\n",
        "#     return file_path, file_name\n",
        "\n",
        "# # Return response sources in formatted string\n",
        "# def create_sources_string(source_urls: Set[str]) -> str:\n",
        "#     if not source_urls:\n",
        "#         return \"\"\n",
        "#     sources_list = list(source_urls)\n",
        "#     sources_list.sort()\n",
        "#     sources_string = \"Pulled from pages:\\n\"\n",
        "#     for i, source in enumerate(sources_list):\n",
        "#         sources_string += f\" {source},\"\n",
        "#     return sources_string\n",
        "\n",
        "# # Return file name for subheadder\n",
        "# @st.cache_resource()\n",
        "# def clean_name(doc_name):\n",
        "\n",
        "#     cleaned_name = re.sub(r'.pdf', '', doc_name, flags=re.IGNORECASE)\n",
        "#     cleaned_name = re.sub(r'\\.', ' ', cleaned_name)\n",
        "#     return cleaned_name\n",
        "\n",
        "####################\n",
        "# Global Variables\n",
        "####################\n",
        "\n",
        "# Creating list of available saved documents\n",
        "saved_docs = list_files()\n",
        "\n",
        "\n",
        "####################\n",
        "# Streamlit interface\n",
        "####################\n",
        "\n",
        "import streamlit as st\n",
        "import time\n",
        "\n",
        "title = st.empty()\n",
        "sub_header = st.empty()\n",
        "\n",
        "title.title('Welcome to Sibyl ðŸ¤–')\n",
        "sub_header.subheader('Your AI assistant for document review!')\n",
        "\n",
        "if (\n",
        "    \"chat_answers_history\" not in st.session_state\n",
        "    and \"user_prompt_history\" not in st.session_state\n",
        "    and \"chat_history\" not in st.session_state\n",
        "):\n",
        "    st.session_state[\"chat_answers_history\"] = []\n",
        "    st.session_state[\"user_prompt_history\"] = []\n",
        "    st.session_state[\"chat_history\"] = []\n",
        "\n",
        "\n",
        "# Function for selecting saved document and returning vectorized database\n",
        "@st.cache_resource(show_spinner='Pulling document from database...')\n",
        "def select_document_sidebar(file):\n",
        "    if file_selected:\n",
        "        outdir = './backend/uploads/'\n",
        "        file_path = os.path.join(outdir, file)\n",
        "        loading_message_container = st.empty()\n",
        "        loading_message_container.info('Hangtight while I search for the document...', icon=\"ðŸ”Ž\")\n",
        "        vectore_store = ingest_doc(file_path, file)\n",
        "        raw_docs = create_doc_obj(file_path, file)\n",
        "        loading_message_container.empty()\n",
        "        return True, vectore_store, raw_docs\n",
        "    return False, None, None\n",
        "\n",
        "# Function for uploading and vectorizing document\n",
        "@st.cache_resource(show_spinner='Processing the document...')\n",
        "def upload_document_sidebar(file):\n",
        "    if file_uploaded:\n",
        "        file_path, file_name = save_upload(file)\n",
        "        loading_message_container = st.empty()\n",
        "        loading_message_container.info(\"Hangtight, I'm giving the document a quick translation into a computer-friendly language. This shouldn't take more than a minute!\",\n",
        "                                       icon=\"ðŸ“‘\")\n",
        "        vectore_store = ingest_doc(file_path, file_name)\n",
        "        raw_docs = create_doc_obj(file_path, file_name)\n",
        "        loading_message_container.empty()\n",
        "        return True, vectore_store, raw_docs\n",
        "    return False, None, None\n",
        "\n",
        "\n",
        "\n",
        "# Sidebar for selecting/uploading document\n",
        "upload_placeholder = st.empty()\n",
        "\n",
        "with upload_placeholder.info(\" ðŸ‘ˆ Select document or upload your own to start chat\"):\n",
        "    st.sidebar.header(\"Select a File or Upload New Document\")\n",
        "    with st.sidebar:\n",
        "\n",
        "        sidebar_completed = False\n",
        "        st.session_state.vectore_store = None\n",
        "\n",
        "        # Adding empty line for spacing\n",
        "        st.markdown(\"\")\n",
        "\n",
        "         # Radio button for user confirmation with agreement link\n",
        "        agreement_link = \"[User Agreement](https://google.com)\"\n",
        "        user_confirmation = st.checkbox(label=f\"I confirm that I have read and understood the {agreement_link}.\")\n",
        "\n",
        "        # Adding empty line for spacing\n",
        "        st.markdown(\"\")\n",
        "\n",
        "        if user_confirmation:  # User confirmed, allow document selection/upload\n",
        "\n",
        "            document_selection = st.radio(\"Would you like to upload a document or select a saved document?\",\n",
        "                             [\"Upload\", \"Select\"],\n",
        "                             captions = [\"Load a new document\", \"Browse preprocessed documents\"])\n",
        "\n",
        "            # Adding empty line for spacing\n",
        "            st.markdown(\"\")\n",
        "\n",
        "            if document_selection == \"Upload\":\n",
        "                # Widget to upload new document\n",
        "                file_uploaded = st.file_uploader(\"Upload your PDF file\", type=\"pdf\", key='FileUpload')\n",
        "                upload_sidebar_completed, uploaded_vectore_store, raw_document_object = upload_document_sidebar(file_uploaded)\n",
        "\n",
        "                if file_uploaded:\n",
        "                    doc_name = file_uploaded.name\n",
        "                    # Successful message\n",
        "                    message_container = st.empty()\n",
        "                    message_container.success('Document processed successfully!', icon=\"âœ…\")\n",
        "                    st.session_state.message_container = message_container\n",
        "\n",
        "                    # Changing control variable to enable chatting\n",
        "                    st.session_state.vectore_store = uploaded_vectore_store\n",
        "                    st.session_state.document_object = raw_document_object\n",
        "                    sidebar_completed = upload_sidebar_completed\n",
        "                    # upload_placeholder.empty()\n",
        "\n",
        "            elif document_selection == \"Select\":\n",
        "                # Create a dropdown menu in the sidebar for file selection\n",
        "                file_selected = st.sidebar.selectbox(label=\"Select a File\", options=saved_docs, placeholder='Choose an option', index=None )\n",
        "                select_sidebar_completed, selected_vectore_store, raw_document_object = select_document_sidebar(file_selected)\n",
        "\n",
        "                if file_selected:\n",
        "                    doc_name = file_selected\n",
        "                    # Successful message\n",
        "                    message_container = st.empty()\n",
        "                    message_container.success('Document loaded!', icon=\"âœ…\")\n",
        "                    st.session_state.message_container = message_container\n",
        "\n",
        "                    # Changing control variable to enable chatting\n",
        "                    st.session_state.vectore_store = selected_vectore_store\n",
        "                    st.session_state.document_object = raw_document_object\n",
        "                    sidebar_completed = select_sidebar_completed\n",
        "                    # upload_placeholder.empty()\n",
        "            else:\n",
        "                st.info(\"Let's pick a document to review!\", icon=\"â˜ï¸\")\n",
        "\n",
        "\n",
        "# Summarize or chat selection\n",
        "if sidebar_completed:\n",
        "    title.title('Sibyl ðŸ¤– is ready to go!')\n",
        "    sub_header.subheader(f\"Let's talk about {clean_name(doc_name)}\")\n",
        "    upload_placeholder.info(\"I've got your document ready!\", icon=\"ðŸ‘‡\")\n",
        "    st.divider()\n",
        "    st.write(\"Get a summary or a personalized checklist of action items by simply clicking the buttons provided below.\")\n",
        "\n",
        "    col1, col2 = st.columns(2)\n",
        "    with col1:\n",
        "        summarize = st.button(\"Summarize\")\n",
        "    with col2:\n",
        "        quick_list = st.button(\"Quick Guidance\")\n",
        "\n",
        "\n",
        "\n",
        "# Summarizing document\n",
        "if \"vectore_store\" in st.session_state is not None and sidebar_completed and summarize:\n",
        "    summary = create_or_load_summ(_doc_object=st.session_state.document_object, doc_name=doc_name)\n",
        "    st.write(summary)\n",
        "\n",
        "# Checklist list\n",
        "if \"vectore_store\" in st.session_state is not None and sidebar_completed and quick_list:\n",
        "    checklist = create_or_load_checklist(_doc_object=st.session_state.document_object, doc_name=doc_name)\n",
        "    st.write(checklist)\n",
        "\n",
        "\n",
        "# Starting chat\n",
        "if \"vectore_store\" in st.session_state is not None and sidebar_completed:\n",
        "    vectore_store = st.session_state.vectore_store\n",
        "\n",
        "    # Adding empty line for spacing\n",
        "    st.markdown(\"\")\n",
        "    st.caption(\"Start chatting by entering your question in the query queue at the bottom of the page!\")\n",
        "    st.divider()\n",
        "    prompt = st.chat_input(placeholder=\"Enter your question here...\")\n",
        "\n",
        "    if prompt:\n",
        "        with st.spinner(\"Searching document for the answer...\"):\n",
        "            generated_response, sources = run_llm_chat(vector_database=vectore_store, question=prompt)\n",
        "            formatted_response = (f\"{generated_response} \\n\\n {create_sources_string(set(sources))}\")\n",
        "\n",
        "        st.session_state.chat_history.append((prompt, generated_response))\n",
        "        st.session_state.user_prompt_history.append(prompt)\n",
        "        st.session_state.chat_answers_history.append(formatted_response)\n",
        "\n",
        "\n",
        "    # Displaying generated response with unique keys\n",
        "    if st.session_state[\"chat_answers_history\"]:\n",
        "        for generated_response, user_query in zip(\n",
        "            st.session_state[\"chat_answers_history\"],\n",
        "            st.session_state[\"user_prompt_history\"],\n",
        "        ):\n",
        "            with st.chat_message(\"user\", avatar=\"ðŸ¤”\"):\n",
        "                st.write(user_query)\n",
        "            with st.chat_message(\"ai\", avatar=\"ðŸ¤–\"):\n",
        "                st.write(generated_response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KIsUX4pIT0VP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mI-5XtOVGqd"
      },
      "source": [
        "# Run Streamlit App\n",
        "Code to run Streamit app in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgpa6Y3eVMGg",
        "outputId": "890a7339-ae42-4226-ad10-9deb48b958a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.83.125.52\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 6.345s\n",
            "your url is: https://six-roses-kneel.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run /content/app.py &>/content/logs.txt & curl ipv4.icanhazip.com\n",
        "!npx localtunnel --port 8501\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u2uE-QqXhK7"
      },
      "source": [
        "# Old Code"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}